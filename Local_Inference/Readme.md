<h1 align="center">App By Local Inference</h1>

<h3>Overview</h3>
ğŸ‘¨â€ğŸ’» In this project, We try to use chosen Model(s) on our Local Server to create ChatBot. No need apikeys in the project. As in other projects, streamlit library has been used for UI concept in this project.
<hr>

<!-- ![Alt text](https://giphy.com/peekasso)  -->

<!-- ------------------------------------------------------ -->

## Folder Skeleton 

```
Folder
|
- localhelper.py (aux module)
- local_chat.py (main file)
- requirements.txt (py libs names to be used)
```

<!-- --------------------------------------- -->

## Objective

ğŸ¯ Know-How Sharing

### The folder covers;

- .py and icon files 

### During the project, note that these issues;
- Download and Install Ollama and LM Studio into your Local Server
- use virtual env (in Terminal => python -m venv "your_virtual_env_name" )
- activate venv (in Terminal => ".\venv_name\Scripts\activate")
- Install dependencies (in Terminal => pip install -r requirements.txt)
- Run the LocalInference Software (in Terminal => ollama run "your_model_name")
- Run the project main file (in Terminal => python local_chat.py)
- <i><u>NOTE THAT base_url=''</u></i> if you use Workstation Server or more advanced one instead of your PC
<hr>

## Author

ğŸ‘¤ **Yasin Sutoglu**

- Twitter: [@YsnStgl](https://twitter.com/YsnStgl)
- Github: [@yasinsutoglu](https://github.com/yasinsutoglu)

